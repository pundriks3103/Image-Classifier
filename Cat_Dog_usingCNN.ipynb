{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cat_Dog_usingCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjIwQLgq5AZP"
      },
      "source": [
        "# **Cat v/s Dog Classification**\n",
        "---\n",
        "**Problem Statement :** This is the very basic problem of classification of Images of two categories using the Deep Neural Networks.\n",
        "\n",
        "Image classification is the process of taking an input (like a picture) and outputting a class (like “cat”) or a probability that the input is a particular class (“there’s a 90% probability that this input is a cat”).\n",
        "\n",
        "The Dataset can be found easily on Kaggle or any other website. This dataset consists of 4000 Images each of Cat and Dog in the Training Set and 1000 Images each of Cat and Dog in the Test Set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4OXd0u9cWq"
      },
      "source": [
        "Image Classification can be solved with the help of a Convolutional Neural Network(CNN). A CNN usually have:\n",
        "\n",
        "1. Convolutional Layers\n",
        "2. ReLU Layers\n",
        "3. Pooling Layers\n",
        "4. Fully connected Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Emp4V967i2"
      },
      "source": [
        "---\n",
        "### Uploading Dataset and Unzipping the Files\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYqMg0OeLThm",
        "outputId": "bbd7e567-582e-4ee6-fbd3-1652eb476c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLtEXMhH7lZ"
      },
      "source": [
        "#Unzipping the Files\n",
        "from zipfile import ZipFile\n",
        "file_name='/content/drive/My Drive/P16-Convolutional-Neural-Networks.zip'\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLlDSVH07IEb"
      },
      "source": [
        "##**Steps of Solution :**\n",
        "\n",
        "1. Importing the required Libraries\n",
        "2. Image Augmentation\n",
        "3. Building the CNN Models.\n",
        "4. Fitting the CNN Models to the Images and finding their Accuracies.\n",
        "5. Removing Overfitting problem by Dropout Regularization\n",
        "6. Saving the Best Model.\n",
        "7. Making Sample Predictions.\n",
        "\n",
        "\n",
        "\n",
        "> **Note :** This problem of Image Classification has no traditional **\"X\"** & **\"y\"** variables although the solution of the problem is a **supervised learning technique**. Here input X is the image itself and output y is the label \"Cat\" or \"Dog\". \n",
        "\n",
        "*   If the Dataset is already labeled and segregated as follows, then we can proceed to the solution directly.\n",
        "\n",
        "```\n",
        "dataset_dogs_vs_cats\n",
        "  ── test\n",
        "      ── cats    \n",
        "      ── dogs\n",
        "  ── train\n",
        "      ── cats\n",
        "      ── dogs\n",
        "```\n",
        "*   If the Dataset has just two folders, 'Train' and 'Test', each containing unlabeled images of cats and dogs, we first need to organise the data as the previous case. This can be simply done using the Keras Library.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7b1VIw37aCC"
      },
      "source": [
        "---\n",
        "### Step 1. Importing the Libraries\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVgRgesC6saS",
        "outputId": "628d8a62-f89c-493e-f437-d25b78a274f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential # To initialise the Neural Network\n",
        "from keras.layers import Convolution2D # To add Convolutional Layers\n",
        "from keras.layers import MaxPooling2D # For Pooling Step\n",
        "from keras.layers import Flatten # For converting Pooled features to Matrix\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRr6UP_M5A6X"
      },
      "source": [
        "---\n",
        "### Step 2. Image Augmentation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0wUPiveUWNw"
      },
      "source": [
        "To train a CNN model more accurately on the image dataset, we can either increase the number of images in the dataset or apply **Image Augmentation**.\n",
        "\n",
        "Image augmentation will create many batches of our images, and in each batch it will apply some random transformation on a random selection of the images, like rotation them, flipping them etc. ; and eventually we will get a large diversification in the batches, and therefore a lot of material to train.\n",
        "\n",
        "It can also act as a regularization technique, adding noise to the training data, and encouraging the model to learn the same features, invariant to their position in the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNcS3WPu8Y9K"
      },
      "source": [
        "**ImageDataGenerator** is used to scale the images (similar to feature scaling in ML). It is scaled by 255 because the values of pixels range from 0-255 for 8-bit data. \n",
        "\n",
        "**flow_from_directory** is used to convert all the images into same size of (64x64), and using batch size of 32 means the weights of neural network will be updated after traing on each batch of 32 images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QFUy2PcDPaZ",
        "outputId": "b79f92a5-3361-4ed7-c105-286c162897db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory('/content/Convolutional_Neural_Networks/dataset/training_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')\n",
        "test_set = test_datagen.flow_from_directory('/content/Convolutional_Neural_Networks/dataset/test_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpCwc-oBRlO"
      },
      "source": [
        "---\n",
        "### Step 3. Building the CNN Models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5717X-Un-scP"
      },
      "source": [
        "A CNN Model\n",
        "*   starts with an input image\n",
        "*   **Convolution Layer:** applies many different filters to it to create a feature map and applies a ReLU function to increase non-linearity.\n",
        "*   **Pooling Layer:** applies a pooling layer to each feature map.\n",
        "    \n",
        "    > The size of Pooling Layer is usually taken as 2x2 to avoid loosing features of the feature map. \n",
        "*   **Flattening Layer:** flattens the pooled images into one long vector.\n",
        "*   **Fully Connected Layer:** inputs the vector into a fully connected artificial neural network.\n",
        "*   processes the features through the network. The final fully connected layer provides the “voting” of the classes that we’re after.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5OdteM9DxCj"
      },
      "source": [
        "    A CNN model may have many number of Convolutional Layers. Standard Procedure involves taking 32, (3x3) filters. \n",
        "\n",
        "The number of filters are increased as the number of Convolutional layers are increased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1j6P2C0G4eN"
      },
      "source": [
        "# Single Convolution Layer CNN Model\n",
        "def cnn_model1():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Flatten())\n",
        "  classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "  classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "  # Compiling the CNN\n",
        "  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "# Two Convolution Layer CNN Model\n",
        "def cnn_model2():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Convolution2D(64, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Flatten())\n",
        "  classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "  classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "  # Compiling the CNN\n",
        "  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "# Three Convolution Layer CNN Model\n",
        "def cnn_model3():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Convolution2D(64, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Convolution2D(128, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Flatten())\n",
        "  classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "  classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "  # Compiling the CNN\n",
        "  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OmigMYV5a0O"
      },
      "source": [
        "---\n",
        "### Step 4. Fitting the CNN Models to the Images and finding their Accuracies.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arRy-ZG4Dp7R"
      },
      "source": [
        "A CNN trains through forward propagation and backpropagation for many, many epochs. This repeats until we have a well-defined neural network with trained weights and feature detectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB-eCOgNLyfr",
        "outputId": "d4461af9-8986-48b5-9b74-e2f5cadc6de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# Fitting the CNN Model_1 to the images\n",
        "model1 = cnn_model1()\n",
        "history1 = model1.fit_generator(training_set, steps_per_epoch = 8000, epochs = 2, validation_data = test_set, validation_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "8000/8000 [==============================] - 1011s 126ms/step - loss: 0.3722 - acc: 0.8265 - val_loss: 0.7882 - val_acc: 0.7446\n",
            "Epoch 2/2\n",
            "8000/8000 [==============================] - 1037s 130ms/step - loss: 0.1224 - acc: 0.9535 - val_loss: 1.0571 - val_acc: 0.7606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp-ZFpUrcqWY",
        "outputId": "1dd495f9-b31e-4703-f1a8-3985ee5b5463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# Fitting the CNN Model_2 to the images\n",
        "model2 = cnn_model2()\n",
        "history2 = model2.fit_generator(training_set, steps_per_epoch = 8000, epochs = 2, validation_data = test_set, validation_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "8000/8000 [==============================] - 1024s 128ms/step - loss: 0.3608 - acc: 0.8312 - val_loss: 0.5475 - val_acc: 0.8055\n",
            "Epoch 2/2\n",
            "8000/8000 [==============================] - 1004s 125ms/step - loss: 0.1381 - acc: 0.9453 - val_loss: 0.7736 - val_acc: 0.8199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLfFTQSGcq8y",
        "outputId": "5fd768b2-f451-4115-e160-b403207e67b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "# Fitting the CNN Model_3 to the images\n",
        "model3 = cnn_model3()\n",
        "history3 = model3.fit_generator(training_set, steps_per_epoch = 8000, epochs = 2, validation_data = test_set, validation_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "8000/8000 [==============================] - 996s 124ms/step - loss: 0.3275 - acc: 0.8487 - val_loss: 0.4287 - val_acc: 0.8470\n",
            "Epoch 2/2\n",
            "8000/8000 [==============================] - 1001s 125ms/step - loss: 0.0972 - acc: 0.9622 - val_loss: 0.5905 - val_acc: 0.8558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAKyqV8d2nhT"
      },
      "source": [
        "acc_val1 = history1.history['val_acc'][1]\n",
        "acc1 = history1.history['acc'][1]\n",
        "acc_val2 = history2.history['val_acc'][1]\n",
        "acc2 = history2.history['acc'][1]\n",
        "acc_val3 = history3.history['val_acc'][1]\n",
        "acc3 = history3.history['acc'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqNIlDEUe_Kb"
      },
      "source": [
        "> **Comparison of Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZMcoNmpe9YW",
        "outputId": "f21fd6ef-62cc-475f-a657-cd62725c168d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Name: CNN with-', '1 Convolution Layer', '2 Convolution Layer', '3 Convolution Layer'])\n",
        "t.add_row(['Accuracy', acc1, acc2, acc3])\n",
        "t.add_row(['Validation Accuracy', acc_val1, acc_val2, acc_val3])\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+---------------------+---------------------+---------------------+\n",
            "|   Name: CNN with-   | 1 Convolution Layer | 2 Convolution Layer | 3 Convolution Layer |\n",
            "+---------------------+---------------------+---------------------+---------------------+\n",
            "|       Accuracy      |     0.9534609375    |    0.94528515625    |    0.96220703125    |\n",
            "| Validation Accuracy |  0.7606476814516129 |  0.8199187247983871 |  0.8558310231854839 |\n",
            "+---------------------+---------------------+---------------------+---------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jQVvVcxP5o4"
      },
      "source": [
        "As, we can see that Model with 3 convolutional layer has maximum accuracy on the validation set, but it seems to have Overfit on the training set. So, the best way is to add dropout regularisation on this model and reduce overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zom5sKv0Qo1E"
      },
      "source": [
        "---\n",
        "### Step 5. Removing Overfitting problem by Dropout Regularization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yitRLMX_Qmzh"
      },
      "source": [
        "# Three Convolution Layer CNN Model (Modified)\n",
        "def cnn_model_modified():\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Convolution2D(32, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Dropout(0.2))\n",
        "  classifier.add(Convolution2D(64, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Dropout(0.2))\n",
        "  classifier.add(Convolution2D(128, 3, 3, input_shape=(64, 64, 3), activation = 'relu'))\n",
        "  classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  classifier.add(Dropout(0.2))\n",
        "  classifier.add(Flatten())\n",
        "  classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "  classifier.add(Dropout(0.5))\n",
        "  classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "  # Compiling the CNN\n",
        "  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx6zb7OMSVGn",
        "outputId": "46f3bc8d-b208-4474-d020-7843f120db42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "# Fitting the CNN Model_Modified to the images\n",
        "model = cnn_model_modified()\n",
        "accuracies = model.fit_generator(training_set, steps_per_epoch = 8000, epochs = 5, validation_data = test_set, validation_steps = 2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "8000/8000 [==============================] - 1024s 128ms/step - loss: 0.4328 - acc: 0.7901 - val_loss: 0.3660 - val_acc: 0.8483\n",
            "Epoch 2/5\n",
            "8000/8000 [==============================] - 1005s 126ms/step - loss: 0.2758 - acc: 0.8823 - val_loss: 0.3475 - val_acc: 0.8680\n",
            "Epoch 3/5\n",
            "8000/8000 [==============================] - 1028s 128ms/step - loss: 0.2151 - acc: 0.9115 - val_loss: 0.3583 - val_acc: 0.8699\n",
            "Epoch 4/5\n",
            "8000/8000 [==============================] - 1035s 129ms/step - loss: 0.1809 - acc: 0.9270 - val_loss: 0.3560 - val_acc: 0.8697\n",
            "Epoch 5/5\n",
            "8000/8000 [==============================] - 1042s 130ms/step - loss: 0.1576 - acc: 0.9372 - val_loss: 0.4168 - val_acc: 0.8757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49f7NzQ5tj0"
      },
      "source": [
        "---\n",
        "### Step 5. Saving the Best Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmJ120kJbBlC"
      },
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('model.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRBEYwFh51ca"
      },
      "source": [
        "---\n",
        "### Step 6. Making Sample Predictions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73dRbnY6MJ4I"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('/content/Convolutional_Neural_Networks/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "actual_test_image = test_image\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model3.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxVtt1AIOtN_",
        "outputId": "225ff78c-525e-4915-fdb5-5cb11c6c3dc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# Viewing the Actual Test Image\n",
        "print(\"The actual image is :-\")\n",
        "actual_test_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual image is :-\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAjZ0lEQVR4nD26abSm11Xnt4dznvEd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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F0E72DBD2B0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWeUF0xJOv6l",
        "outputId": "374d7bff-f008-4ea8-ef79-7e6204cfde5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Viewing the Prediction\n",
        "print(\"The predicted image is a\", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted image is a dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_l-5Jtp9NrJ"
      },
      "source": [
        "**Conclusion :** A random image of cat or dog, can thus be predicted with 87.95 % Accuracy. "
      ]
    }
  ]
}